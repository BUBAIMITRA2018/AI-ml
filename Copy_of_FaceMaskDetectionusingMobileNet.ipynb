{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FaceMaskDetectionusingMobileNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyML/7q5QrdGg8BbZ3bpZShN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BUBAIMITRA2018/AI-ml/blob/master/Copy_of_FaceMaskDetectionusingMobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z15IkwPSHniR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7f6134-7aac-424d-83a1-4355eff5ba13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWgh05gqMHwX"
      },
      "source": [
        "path = '/content/drive/My Drive/Face_Mask_Detection/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aEV6wr3RbpP"
      },
      "source": [
        "DATAPATH = '/content/drive/My Drive/Face_Mask_Detection/'\n",
        "MASKPATH = '/content/drive/My Drive/Face_Mask_Detection/WithMask/'\n",
        "NOMASKPATH = '/content/drive/My Drive/Face_Mask_Detection/WithoutMask/'\n",
        "TESTPATH = '/content/drive/My Drive/Face_Mask_Detection/testdata'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFD9h6cV8Fd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D, Flatten, Dropout, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "523XJXCIWAJd"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9_u27fPXJGf"
      },
      "source": [
        "def getTest(pth):\n",
        "    dataSplit = int(np.ceil(len(os.listdir(pth))*0.02))\n",
        "    for img in os.listdir(pth)[-dataSplit:]:\n",
        "        shutil.move(os.path.join(pth,img), os.path.join('testdata'.join(pth.split('data')),img))\n",
        "getTest(MASKPATH)\n",
        "getTest(NOMASKPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHNOTPAGXYdt",
        "outputId": "6f7ad41b-f01b-4413-8158-eb410ed92ae3"
      },
      "source": [
        "len(os.listdir(MASKPATH)),len(os.listdir(NOMASKPATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(690, 686)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWhIUhVVYNXJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubjrR4VrYVx8"
      },
      "source": [
        "Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZBKmJcsYbMb"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inaaJ-p0Ygj-"
      },
      "source": [
        "\n",
        "trainGen = ImageDataGenerator(\n",
        "    rescale= 1/255.,\n",
        "    horizontal_flip=True,\n",
        "    validation_split = 0.1\n",
        ")\n",
        "\n",
        "testGen = ImageDataGenerator(\n",
        "    rescale= 1/255.,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfhDPTAYlk8",
        "outputId": "cd9af828-6694-459b-f46b-a57678475093"
      },
      "source": [
        "train = trainGen.flow_from_directory(\n",
        "    DATAPATH, \n",
        "    target_size=(224, 224),\n",
        "    classes=['WithMask','WithoutMask'],\n",
        "    class_mode='categorical', \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation = trainGen.flow_from_directory(\n",
        "    DATAPATH, \n",
        "    target_size=(224, 224),\n",
        "    classes=['WithMask','WithoutMask'],\n",
        "    class_mode='categorical', \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test = testGen.flow_from_directory(\n",
        "    TESTPATH, \n",
        "    target_size=(224, 224), \n",
        "    classes=['with_mask','without_mask'],\n",
        "    class_mode='categorical', \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1239 images belonging to 2 classes.\n",
            "Found 137 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFmlgIESZYcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3b6748-9561-4fd6-f7a7-439afdccf2dc"
      },
      "source": [
        "mob = MobileNetV2(\n",
        "    input_shape = (224,224,3),\n",
        "    include_top = False,\n",
        "    weights = 'imagenet',\n",
        ")\n",
        "mob.trainable = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRwKDgGXZ1ib",
        "outputId": "d903701c-4cf1-45c9-dc64-33309b302c46"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(mob)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                81984     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 2,340,098\n",
            "Trainable params: 82,114\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98GB6VAZaK3N"
      },
      "source": [
        "\n",
        "model.compile(optimizer=Adam(),loss='categorical_crossentropy',metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KICOc7W5aMu8"
      },
      "source": [
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'model.h5',\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrqHAvocaVfE",
        "outputId": "fed0a244-d89f-4a91-f8a0-8d3908e61d9f"
      },
      "source": [
        "hist = model.fit(\n",
        "    train,\n",
        "    epochs = 15,\n",
        "    validation_data = validation,\n",
        "    callbacks = [checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1276 - acc: 0.9443\n",
            "Epoch 00001: val_loss improved from inf to 0.01992, saving model to model.h5\n",
            "39/39 [==============================] - 269s 7s/step - loss: 0.1276 - acc: 0.9443 - val_loss: 0.0199 - val_acc: 0.9927\n",
            "Epoch 2/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0126 - acc: 0.9952\n",
            "Epoch 00002: val_loss improved from 0.01992 to 0.01945, saving model to model.h5\n",
            "39/39 [==============================] - 7s 170ms/step - loss: 0.0126 - acc: 0.9952 - val_loss: 0.0194 - val_acc: 0.9927\n",
            "Epoch 3/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9992\n",
            "Epoch 00003: val_loss improved from 0.01945 to 0.01451, saving model to model.h5\n",
            "39/39 [==============================] - 7s 171ms/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.0145 - val_acc: 1.0000\n",
            "Epoch 4/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
            "Epoch 00004: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 164ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0285 - val_acc: 0.9854\n",
            "Epoch 5/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 00005: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 7s 169ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0298 - val_acc: 0.9854\n",
            "Epoch 6/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 00006: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9927\n",
            "Epoch 7/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9992\n",
            "Epoch 00007: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0314 - val_acc: 0.9854\n",
            "Epoch 8/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 00008: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9927\n",
            "Epoch 9/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
            "Epoch 00009: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 160ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 0.0239 - val_acc: 0.9927\n",
            "Epoch 10/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 5.9252e-04 - acc: 1.0000\n",
            "Epoch 00010: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 166ms/step - loss: 5.9252e-04 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 0.9927\n",
            "Epoch 11/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 5.7822e-04 - acc: 1.0000\n",
            "Epoch 00011: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 5.7822e-04 - acc: 1.0000 - val_loss: 0.0215 - val_acc: 0.9927\n",
            "Epoch 12/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 4.4646e-04 - acc: 1.0000\n",
            "Epoch 00012: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 166ms/step - loss: 4.4646e-04 - acc: 1.0000 - val_loss: 0.0192 - val_acc: 0.9927\n",
            "Epoch 13/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 5.7252e-04 - acc: 1.0000\n",
            "Epoch 00013: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 5.7252e-04 - acc: 1.0000 - val_loss: 0.0210 - val_acc: 0.9854\n",
            "Epoch 14/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
            "Epoch 00014: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 165ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0177 - val_acc: 0.9927\n",
            "Epoch 15/15\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
            "Epoch 00015: val_loss did not improve from 0.01451\n",
            "39/39 [==============================] - 6s 166ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0273 - val_acc: 0.9854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Gp9znmFIQe"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/Face_Mask_Detection/mask_detector.model\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "C0XeUbebZj7G",
        "outputId": "4b2f1147-c0bb-4ba7-9a32-a28e53b4f498"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\tprint(detections.shape)\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the detection\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > 0.5:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tfaces.append(face)\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "\n",
        "\t# only make a predictions if at least one face was detected\n",
        "\tif len(faces) > 0:\n",
        "\t\t# for faster inference we'll make batch predictions on *all*\n",
        "\t\t# faces at the same time rather than one-by-one predictions\n",
        "\t\t# in the above `for` loop\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "\t# return a 2-tuple of the face locations and their corresponding\n",
        "\t# locations\n",
        "\treturn (locs, preds)\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "prototxtPath = r\"/content/drive/My Drive/Face_Mask_Detection/deploy.prototxt\"\n",
        "weightsPath = r\"/content/drive/My Drive/Face_Mask_Detection/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "maskNet = load_model(\"/content/drive/My Drive/Face_Mask_Detection/mask_detector.model\")\n",
        "\n",
        "# initialize the video stream\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# detect faces in the frame and determine if they are wearing a\n",
        "\t# face mask or not\n",
        "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "\t# loop over the detected face locations and their corresponding\n",
        "\t# locations\n",
        "\tfor (box, pred) in zip(locs, preds):\n",
        "\t\t# unpack the bounding box and predictions\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\t(mask, withoutMask) = pred\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] starting video stream...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-980a5d627daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# to have a maximum width of 400 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# detect faces in the frame and determine if they are wearing a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}